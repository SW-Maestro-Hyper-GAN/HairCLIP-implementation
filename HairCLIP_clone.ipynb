{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HairCLIP-clone.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1jIn4MeVz0N",
        "outputId": "0b939fc8-39e1-4f56-f8de-de497b397d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HairCLIP-Implementation/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq1BgIOuWE8L",
        "outputId": "ab1d3e1f-e781-4b0c-938d-b693d2046573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HairCLIP-Implementation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/wty-ustc/HairCLIP.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1mHrFU3WZAA",
        "outputId": "639034b4-7e76-4984-a3b1-893e3a978c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HairCLIP'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 161 (delta 45), reused 155 (delta 40), pack-reused 0\n",
            "Receiving objects: 100% (161/161), 40.31 MiB | 16.57 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall torch torchvision"
      ],
      "metadata": {
        "id": "s9Ea7gysjycJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HairCLIP train 시키기 위한 라이브러리, 모델\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install tensorflow-io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC1izZWabrF9",
        "outputId": "730209c6-a326-47a6-83dc-0d7c7d4fdc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-09o9_zqp\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-09o9_zqp\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=4f07415dcfe3ef90483ac3fb71913203926e511c6bebc700f238d40a26d05f17\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t3js_7b_/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.26.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.26.0)\n",
            "Installing collected packages: tensorflow-io\n",
            "Successfully installed tensorflow-io-0.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd HairCLIP/mapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zEx0RpEWd90",
        "outputId": "1edeaa70-fddc-4b8c-fdca-d6209bc529a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HairCLIP-Implementation/HairCLIP/mapper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/train.py \\\n",
        "--exp_dir=../path/to/experiment \\\n",
        "--hairstyle_description=\"hairstyle_list.txt\" \\\n",
        "--color_description=\"purple, red, orange, yellow, green, blue, gray, brown, black, white, blond, pink\" \\\n",
        "--latents_train_path=../pretrained_models/train_faces.pt \\\n",
        "--latents_test_path=../pretrained_models/test_faces.pt \\\n",
        "--hairstyle_ref_img_train_path=../celeba_hq/train \\\n",
        "--hairstyle_ref_img_test_path=../celeba_hq/val \\\n",
        "--color_ref_img_train_path=../celeba_hq/train \\\n",
        "--color_ref_img_test_path=../celeba_hq/val \\\n",
        "--hairstyle_manipulation_prob=0.5 \\\n",
        "--color_manipulation_prob=0.2 \\\n",
        "--both_manipulation_prob=0.27 \\\n",
        "--hairstyle_text_manipulation_prob=0.5 \\\n",
        "--color_text_manipulation_prob=0.5 \\\n",
        "--color_in_domain_ref_manipulation_prob=0.25 \\\n",
        "# --color_ref_img_in_domain_path=../path/to/generated_hair_of_various_colors \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3a8U0qSZCbB",
        "outputId": "d2c81b0d-ac5a-4a0b-8380-fd669a740a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attribute_preservation_lambda': 1.0,\n",
            " 'background_lambda': 1.0,\n",
            " 'batch_size': 1,\n",
            " 'board_interval': 50,\n",
            " 'both_manipulation_prob': 0.27,\n",
            " 'checkpoint_path': None,\n",
            " 'color_description': 'purple, red, orange, yellow, green, blue, gray, brown, '\n",
            "                      'black, white, blond, pink',\n",
            " 'color_in_domain_ref_manipulation_prob': 0.25,\n",
            " 'color_manipulation_prob': 0.2,\n",
            " 'color_ref_img_in_domain_path': '',\n",
            " 'color_ref_img_test_path': '../celeba_hq/val',\n",
            " 'color_ref_img_train_path': '../celeba_hq/train',\n",
            " 'color_text_manipulation_prob': 0.5,\n",
            " 'exp_dir': '../path/to/experiment',\n",
            " 'hairstyle_description': 'hairstyle_list.txt',\n",
            " 'hairstyle_manipulation_prob': 0.5,\n",
            " 'hairstyle_ref_img_test_path': '../celeba_hq/val',\n",
            " 'hairstyle_ref_img_train_path': '../celeba_hq/train',\n",
            " 'hairstyle_text_manipulation_prob': 0.5,\n",
            " 'id_lambda': 0.3,\n",
            " 'image_color_lambda': 0.02,\n",
            " 'image_hairstyle_lambda': 5.0,\n",
            " 'image_interval': 100,\n",
            " 'image_manipulation_lambda': 1.0,\n",
            " 'ir_se50_weights': '../pretrained_models/model_ir_se50.pth',\n",
            " 'latent_l2_lambda': 0.8,\n",
            " 'latents_test_path': '../pretrained_models/test_faces.pt',\n",
            " 'latents_train_path': '../pretrained_models/train_faces.pt',\n",
            " 'learning_rate': 0.0005,\n",
            " 'maintain_color_lambda': 0.02,\n",
            " 'max_steps': 500000,\n",
            " 'no_coarse_mapper': False,\n",
            " 'no_fine_mapper': False,\n",
            " 'no_medium_mapper': False,\n",
            " 'num_for_each_augmented_color': 4000,\n",
            " 'optim_name': 'ranger',\n",
            " 'parsenet_weights': '../pretrained_models/parsenet.pth',\n",
            " 'save_interval': 2000,\n",
            " 'stylegan_size': 1024,\n",
            " 'stylegan_weights': '../pretrained_models/stylegan2-ffhq-config-f.pt',\n",
            " 'test_batch_size': 1,\n",
            " 'test_dataset_size': 1000,\n",
            " 'test_workers': 2,\n",
            " 'text_manipulation_lambda': 2.0,\n",
            " 'train_dataset_size': 5000,\n",
            " 'val_interval': 2000,\n",
            " 'workers': 4}\n",
            "Loading decoder weights from pretrained!\n",
            "Loading ResNet ArcFace for ID Loss\n",
            "Loading UNet for Background Loss\n",
            "Loading UNet for AvgLabLoss\n",
            "Loading UNet for AvgLabLoss\n",
            "Number of training samples: 24176\n",
            "Number of test samples: 2824\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "../mapper/training/ranger.py:123: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:69: UserWarning: The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. Please use 'value_range' instead.\n",
            "  \"The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. \"\n",
            "Metrics for train, step 0 mohawk hairstyle-red hair\n",
            "\tloss_id =  0.08145904541015625\n",
            "\tid_improve =  0.0\n",
            "\tloss_text_hairstyle =  0.81396484375\n",
            "\tloss_text_color =  0.83203125\n",
            "\tloss_background =  0.004990512505173683\n",
            "\tloss_l2_latent =  0.006117294076830149\n",
            "\tloss =  3.3263142108917236\n",
            "feature shape:  torch.Size([1, 512])\n",
            "tensor_feature shape:  torch.Size([1, 512])\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/train.py\", line 34, in <module>\n",
            "    main(opts)\n",
            "  File \"scripts/train.py\", line 27, in main\n",
            "    coach.train()\n",
            "  File \"../mapper/training/coach.py\", line 116, in train\n",
            "    val_loss_dict = self.validate()\n",
            "  File \"../mapper/training/coach.py\", line 162, in validate\n",
            "    loss, cur_loss_dict = self.calc_loss(w, x, w_hat, x_hat, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor, color_tensor, selected_description)\n",
            "  File \"../mapper/training/coach.py\", line 266, in calc_loss\n",
            "    loss_img_hairstyle = self.image_embedding_loss((x_hat * self.average_color_loss.gen_hair_mask(x_hat)), (hairstyle_tensor * self.average_color_loss.gen_hair_mask(hairstyle_tensor))).mean()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"../criteria/image_embedding_loss.py\", line 26, in forward\n",
            "    similarity = self.cosloss(masked_generated_feature, masked_img_tensor_feature, cos_target).unsqueeze(0).unsqueeze(0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\", line 1266, in forward\n",
            "    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 3478, in cosine_embedding_loss\n",
            "    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\n",
            "RuntimeError: 0D or 1D target tensor expected, multi-target not supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JJzcxLmIjiR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}